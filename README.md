# Network compression and Acceleration Papers

# Survey papers

  1. **Efficient processing of deep neural networks: A tutorial and survey.** *Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer.* [paper](https://ieeexplore.ieee.org/abstract/document/8114708)
  2. **A survey of model compression and acceleration for deep neural networks.** *Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang.* [paper](https://arxiv.org/abs/1710.09282)
  3. **An analysis of deep neural network models for practical applications.** *Alfredo Canziani, Adam Paszke, and Eugenio Culurciello.* [paper](https://arxiv.org/abs/1605.07678)
  4. **Deep convolutional neural networks for image classiffication: A comprehensive review.** *Waseem Rawat and Zenghui Wang.* [paper](https://www.mitpressjournals.org/doi/full/10.1162/neco_a_00990)

# Knowledge distilling

  1. **Distilling the knowledge in a neural network.** *Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.* [paper]()
  2. **Fitnets: Hints for thin deep nets.** *Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.* 
  3. **Harnessing deep neural networks with logic rules.** *Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing.* 
  4. **Do deep nets really need to be deep?** *Jimmy Ba and Rich Caruana.* 
  5. **Do deep convolutional nets really need to be deep and convolutional?** *Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson.* 
  6. **Transferring knowledge from a rnn to a dnn.** *William Chan, Nan Rosemary Ke, and Ian Lane.* 
  7. **Face model compression by distilling knowledge from neurons.** *Ping Luo, Zhenyao Zhu, Ziwei Liu, Xiaogang Wang, Xiaoou Tang, et al.* 
  8. **Like what you like: Knowledge distill via neuron selectivity transfer.** *Zehao Huang and Naiyan Wang.* 
  9. **Darkrank: Accelerating deep metric learning via cross sample similarities transfer.** *Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.* 
  10. **Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.** *Sergey Zagoruyko and Nikos Komodakis.*
  11. **Accelerating convolutional neural networks with dominant convolutional kernel and knowledge pre-regression.** *Zhenyang Wang, Zhidong Deng, and Shiyao Wang.*
  12. **Rocket launching: A universal and efficient framework for training well-performing light net.** *Guorui Zhou, Ying Fan, Runpeng Cui, Weijie Bian, Xiaoqiang Zhu, and Kun Gai.* 
  
# Network pruning
  1. **Optimal brain damage.** *Yann LeCun, John S Denker, and Sara A Solla.*
  1. **Second order derivatives for network pruning: Optimal brain surgeon.** *Babak Hassibi, David G Stork, et al.*
  1. 

# Network quantization

# Low rank approximation
  1. **Predicting parameters in deep learning.** *Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al.*
  1. **Learning both weights and connections for efficient neural network.** *Song Han, Jeff Pool, John Tran, and William Dally.* 
  1. **Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.** *Song Han, Huizi Mao, and William J Dally.*
  1. **Dsd: Dense-sparse-dense training for deep neural networks.** *Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, et al.*
  1. **Data-free parameter pruning for deep neural networks.** *Suraj Srinivas and R Venkatesh Babu.*
  1. **Dynamic network surgery for efficient dnns.** *Yiwen Guo, Anbang Yao, and Yurong Chen.*
  

# Dynamic computation

# Compact network design

# Others

