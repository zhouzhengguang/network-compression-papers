# Network compression and Acceleration Papers

# Survey papers
  1. **Efficient processing of deep neural networks: A tutorial and survey.** *Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer.* [paper](https://ieeexplore.ieee.org/abstract/document/8114708)
  2. **A survey of model compression and acceleration for deep neural networks.** *Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang.* [paper](https://arxiv.org/abs/1710.09282)
  3. **An analysis of deep neural network models for practical applications.** *Alfredo Canziani, Adam Paszke, and Eugenio Culurciello.* [paper](https://arxiv.org/abs/1605.07678)
  4. **Deep convolutional neural networks for image classiffication: A comprehensive review.** *Waseem Rawat and Zenghui Wang.* [paper](https://www.mitpressjournals.org/doi/full/10.1162/neco_a_00990)

# Knowledge distilling
  1. **Distilling the knowledge in a neural network.** *Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.* [paper]()
  2. **Fitnets: Hints for thin deep nets.** *Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.* 
  3. **Harnessing deep neural networks with logic rules.** *Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing.* 
  4. **Do deep nets really need to be deep?** *Jimmy Ba and Rich Caruana.* 
  5. **Do deep convolutional nets really need to be deep and convolutional?** *Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson.* 
  6. **Transferring knowledge from a rnn to a dnn.** *William Chan, Nan Rosemary Ke, and Ian Lane.* 
  7. **Face model compression by distilling knowledge from neurons.** *Ping Luo, Zhenyao Zhu, Ziwei Liu, Xiaogang Wang, Xiaoou Tang, et al.* 
  8. **Like what you like: Knowledge distill via neuron selectivity transfer.** *Zehao Huang and Naiyan Wang.* 
  9. **Darkrank: Accelerating deep metric learning via cross sample similarities transfer.** *Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.* 
  10. **Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.** *Sergey Zagoruyko and Nikos Komodakis.*
  11. **Accelerating convolutional neural networks with dominant convolutional kernel and knowledge pre-regression.** *Zhenyang Wang, Zhidong Deng, and Shiyao Wang.*
  12. **Rocket launching: A universal and efficient framework for training well-performing light net.** *Guorui Zhou, Ying Fan, Runpeng Cui, Weijie Bian, Xiaoqiang Zhu, and Kun Gai.* 
  
# Network pruning
  1. **Optimal brain damage.** *Yann LeCun, John S Denker, and Sara A Solla.*
  1. **Second order derivatives for network pruning: Optimal brain surgeon.** *Babak Hassibi, David G Stork, et al.*
  1. **Learning both weights and connections for efficient neural network.** *Song Han, Jeff Pool, John Tran, and William Dally.* 
  1. **Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.** *Song Han, Huizi Mao, and William J Dally.*
  1. **Dsd: Dense-sparse-dense training for deep neural networks.** *Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian Tang, Erich Elsen, Peter Vajda, Manohar Paluri, John Tran, et al.*
  1. **Data-free parameter pruning for deep neural networks.** *Suraj Srinivas and R Venkatesh Babu.*
  1. **Dynamic network surgery for efficient dnns.** *Yiwen Guo, Anbang Yao, and Yurong Chen.*
  1. **Faster cnns with direct sparse convolutions and guided pruning.** *Jongsoo Park, Sheng Li, Wei Wen, Ping Tak Peter Tang, Hai Li, Yiran Chen, and Pradeep Dubey.*
  1. **Pruning convolutional neural networks for resource efficient transfer learning.** *Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.*
  1. **Pruning filters for efficient convnets.** *Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf.*
  1. **Structured pruning of deep convolutional neural networks.** *Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung.*
  1. **A simple yet effective method to prune dense layers of neural networks.** *Mohammad Babaeizadeh, Paris Smaragdis, and Roy H Campbell.*
  1. **Designing energy-efficient convolutional neural networks using energy-aware pruning.** *Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.*
  1. **Bayesian compression for deep learning.** *Christos Louizos, Karen Ullrich, and Max Welling.*
  1. **Weight uncertainty in neural networks.** *Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.*
  1. **An entropy-based pruning method for cnn compression.** *Jian-Hao Luo and Jianxin Wu.*
  1. **Exploring the regularity of sparse structure in convolutional neural networks.** *Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, and William J Dally.*
  1. **Fast convnets using group-wise brain damage.** *Vadim Lebedev and Victor Lempitsky.*
  1. **Thinet: A filter level pruning method for deep neural network compression.** *Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.*
  1. **Network trimming: A data-driven neuron pruning approach towards efficient deep architectures.** *Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.*
  1. **Accelerating deep learning with shrinkage and recall.** *Shuai Zheng, Abhinav Vishnu, and Chris Ding.*
  1. **Prune the convolutional neural networks with sparse shrink.** *Xin Li and Changsong Liu.*
  1. **Neuron pruning for compressing deep networks using maxout architectures.** *Fernando Moya Rueda, Rene Grzeszick, and Gernot A Fink.*
  1. **Fine-pruning: Joint fine-tuning and compression of a convolutional network with bayesian optimization.** *Frederick Tung, Srikanth Muralidharan, and Greg Mori.*
  1. **Structured bayesian pruning via log-normal multiplicative noise.** *Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.*
  1. **Towards evolutional compression.** *Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, and Dacheng Tao.*
  1. **Lazy evaluation of convolutional filters.** *Sam Leroux, Steven Bohez, Cedric De Boom, Elias De Coninck, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, and Bart Dhoedt.*
  1. **Sparsely-connected neural networks: Towards efficient vlsi implementation of deep neural networks.** *Arash Ardakani, Carlo Condo, andWarren J Gross.*
  1. **Net-trim: A layer-wise convex pruning of deep neural networks.** *Alireza Aghasi, Nam Nguyen, and Justin Romberg.*
  1. **Learning with confident examples: Rank pruning for robust classification with noisy labels.** *Curtis G. Northcutt, Tailin Wu, and Isaac L. Chuang.*
  1. **Compact deep convolutional neural networks with coarse pruning.** *Sajid Anwar and Wonyong Sung.*
  1. **Towards thinner convolutional neural networks through gradually global pruning.** *Zhengtao Wang, Ce Zhu, Zhiqiang Xia, Qi Guo, and Yipeng Liu.*
  1. **The incredible shrinking neural network: New perspectives on learning representations through the lens of pruning.** *Nikolas Wolfe, Aditya Sharma, Lukas Drude, and Bhiksha Raj.*
  1. **Training skinny deep neural networks with iterative hard thresholding methods.** *Xiaojie Jin, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan.*
  1. **Reducing the model order of deep neural networks using information theory.** *Ming Tu, Visar Berisha, Yu Cao, and Jae Sun Seo.*
  1. **Learning efficient convolutional networks through network slimming.** *Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.*
  1. **Channel pruning for accelerating very deep neural networks.** *Yihui He, Xiangyu Zhang, and Jian Sun.*
  1. **Incomplete dot products for dynamic computation scaling in neural network inference.** *H. T. Kung Bradley McDanel, Surat Teerapittayanon.*
  1. **To prune, or not to prune: exploring the efficacy of pruning for model compression.** *Michael Zhu and Suyog Gupta.*
  1. **Data-driven sparse structure selection for deep neural networks.** *Zehao Huang and Naiyan Wang.*
  1. **Pruning convnets online for efficient specialist models.** *Jia Guo and Miodrag Potkonjak.*

# Network quantization
  1. **Fixed-point feedforward deep neural network design using weights+1, 0, and- 1.** *Kyuyeon Hwang and Wonyong Sung.*
  1. **Fixed point quantization of deep convolutional networks.** *Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.*
  1. **Binaryconnect: Training deep neural networks with binary weights during propagations.** *Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.*
  1. **Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.** *Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.*
  1. **Quantized neural networks: Training neural networks with low precision weights and activations.** *Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.*
  1. **Xnor-net: Imagenet classification using binary convolutional neural networks.** *Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.*
  1. **Bitwise neural networks.** *Minje Kim and Paris Smaragdis.*
  1. **Training quantized nets: A deeper understanding.** *Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein.*
  1. **Shiftcnn: Generalized low-precision architecture for inference of convolutional neural networks.** *Denis A. Gudovskiy and Luca Rigazio.*
  1. **Gated xnor networks: Deep neural networks with ternary weights and activations under a unified discretization framework.** *Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, and Guoqi Li.*
  1. **The high-dimensional geometry of binary neural networks.** *Alexander G Anderson and Cory P Berg.*
  1. **Compressing deep convolutional networks using vector quantization.** *Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.*
  1. **Compression of deep neural networks on the fly.** *Guillaume Soulie, Vincent Gripon, and Maelys Robert.*
  1. **Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.** *Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.*
  1. **Ternary weight networks.** *Fengfu Li, Bo Zhang, and Bin Liu.*
  1. **Trained ternary quantization.** *Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally.*
  1. **Incremental network quantization: Towards lossless cnns with low-precision weights.** *Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.*
  1. **Quantized convolutional neural networks for mobile devices.** *Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.*
  1. **Compressing neural networks with the hashing trick.** *Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.*
  1. **Scalable and sustainable deep learning via randomized hashing.** *Ryan Spring and Anshumali Shrivastava.*
  1. **Functional hashing for compressing neural networks.** *Lei Shi, Shikun Feng, et al.*
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **

# Low rank approximation
  1. **Predicting parameters in deep learning.** *Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al.*
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **

# Dynamic computation
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  
# Compact network design
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **

# Others
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
  1. **** **
