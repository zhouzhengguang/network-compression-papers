# Network compression and Acceleration Papers

# Survey papers

  1. **Efficient processing of deep neural networks: A tutorial and survey.** *Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer.* [paper](https://ieeexplore.ieee.org/abstract/document/8114708)
  2. **A survey of model compression and acceleration for deep neural networks.** *Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang.* [paper](https://arxiv.org/abs/1710.09282)
  3. **An analysis of deep neural network models for practical applications.** *Alfredo Canziani, Adam Paszke, and Eugenio Culurciello.* [paper](https://arxiv.org/abs/1605.07678)
  4. **Deep convolutional neural networks for image classiffication: A comprehensive review.** *Waseem Rawat and Zenghui Wang.* [paper](https://www.mitpressjournals.org/doi/full/10.1162/neco_a_00990)

# Knowledge distilling

  1. **Distilling the knowledge in a neural network.** *Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.* [paper]()
  2. **Fitnets: Hints for thin deep nets.** *Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.* [paper]()
  3. **Harnessing deep neural networks with logic rules.** *Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing.* [paper]()
  4. **Do deep nets really need to be deep?** *Jimmy Ba and Rich Caruana.* [paper]()
  5. **Do deep convolutional nets really need to be deep and convolutional?** *Gregor Urban, Krzysztof J Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson.* [paper]()
  6. **Transferring knowledge from a rnn to a dnn.** *William Chan, Nan Rosemary Ke, and Ian Lane.* [paper]()
  7. **Face model compression by distilling knowledge from neurons.** *Ping Luo, Zhenyao Zhu, Ziwei Liu, Xiaogang Wang, Xiaoou Tang, et al.* [paper]()
  8. **Like what you like: Knowledge distill via neuron selectivity transfer.** *Zehao Huang and Naiyan Wang.* 
  9. **Darkrank: Accelerating deep metric learning via cross sample similarities transfer.** *Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.* 
  10. **Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.** *Sergey Zagoruyko and Nikos Komodakis.*
  
# Network pruning

# Network quantization

# Low rank approximation

# Dynamic computation

# Compact network design

# Others

